{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "68c2b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (3.1.2)\n",
      "Requirement already satisfied: lightgbm in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: catboost in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (1.2.8)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from xgboost) (2.3.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from xgboost) (1.16.3)\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (3.10.7)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (2.3.3)\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (6.5.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from plotly->catboost) (2.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "21a304a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6a71f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_available = True\n",
    "except Exception:\n",
    "    xgb_available = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lgb_available = True\n",
    "except Exception:\n",
    "    lgb_available = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    cb_available = True\n",
    "except Exception:\n",
    "    cb_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be2fc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae4adfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##UTILITIES\n",
    "# def rmse(y_true, y_pred):\n",
    "#     return root_mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def evaluate_predictions_log(y_true_log, y_pred_log):\n",
    "    \"\"\"Return metrics on log scale and on original price scale (expm1).\"\"\"\n",
    "    # Log-target metrics\n",
    "    rmse_log = root_mean_squared_error(y_true_log, y_pred_log)\n",
    "    mae_log = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2_log = r2_score(y_true_log, y_pred_log)\n",
    "\n",
    "    # Convert back to price scale\n",
    "    y_true_price = np.expm1(y_true_log)\n",
    "    y_pred_price = np.expm1(y_pred_log)\n",
    "\n",
    "    rmse_price = root_mean_squared_error(y_true_price, y_pred_price)\n",
    "    mae_price = mean_absolute_error(y_true_price, y_pred_price)\n",
    "    r2_price = r2_score(y_true_price, y_pred_price)\n",
    "\n",
    "    return {\n",
    "        \"rmse_log\": rmse_log,\n",
    "        \"mae_log\": mae_log,\n",
    "        \"r2_log\": r2_log,\n",
    "        \"rmse_price\": rmse_price,\n",
    "        \"mae_price\": mae_price,\n",
    "        \"r2_price\": r2_price\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "57634431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_booster_with_fallback(name, model, X_tr_t, y_tr, X_val_t, y_val):\n",
    "    \"\"\"\n",
    "    Try multiple ways to fit booster with early stopping across library versions:\n",
    "      1) model.fit(..., eval_set=[...], early_stopping_rounds=...)\n",
    "      2) model.fit(..., eval_set=[...], callbacks=[...]) (XGBoost / LightGBM callback)\n",
    "      3) CatBoost: use_best_model=True\n",
    "      4) fallback: model.fit(...) without early stopping\n",
    "    Returns: (fitted_model, method_string)\n",
    "    \"\"\"\n",
    "    default_es = 30\n",
    "    # 1) try classic API\n",
    "    try:\n",
    "        model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], early_stopping_rounds=default_es, verbose=False)\n",
    "        return model, \"fit(early_stopping_rounds)\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) try callbacks for XGBoost/LightGBM\n",
    "    try:\n",
    "        if \"XGB\" in name and xgb_available:\n",
    "            try:\n",
    "                # attempt xgboost callback import\n",
    "                from xgboost.callback import EarlyStopping\n",
    "                model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[EarlyStopping(rounds=default_es)])\n",
    "                return model, \"xgb.callback.EarlyStopping\"\n",
    "            except Exception:\n",
    "                # alternate import path\n",
    "                import xgboost as xgbpkg\n",
    "                try:\n",
    "                    model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[xgbpkg.callback.EarlyStopping(rounds=default_es)])\n",
    "                    return model, \"xgbpkg.callback.EarlyStopping\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if \"LightGBM\" in name and lgb_available:\n",
    "            try:\n",
    "                from lightgbm import early_stopping\n",
    "                model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[early_stopping(default_es)])\n",
    "                return model, \"lightgbm.callback.early_stopping\"\n",
    "            except Exception:\n",
    "                # try package callback\n",
    "                try:\n",
    "                    import lightgbm as lgbpkg\n",
    "                    model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[lgbpkg.callback.early_stopping(default_es)])\n",
    "                    return model, \"lgbpkg.callback.early_stopping\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if \"CatBoost\" in name and cb_available:\n",
    "            try:\n",
    "                model.fit(X_tr_t, y_tr, eval_set=(X_val_t, y_val), use_best_model=True, verbose=False)\n",
    "                return model, \"catboost.use_best_model\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) fallback: fit without early stopping\n",
    "    try:\n",
    "        model.fit(X_tr_t, y_tr)\n",
    "        return model, \"fallback_no_early_stopping\"\n",
    "    except Exception as e:\n",
    "        # If it fails to fit at all, raise\n",
    "        raise RuntimeError(f\"Booster {name} failed to fit in all attempts: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bb35e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10682, 29)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('processed_flight_data.csv')\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d8b00d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Detect whether data is OHE or needs preprocessing ----------\n",
    "# If categorical columns exist as names, we will build a ColumnTransformer.\n",
    "categorical_candidates = [\"Airline\", \"Source\", \"Destination\"]\n",
    "has_categorical = any(c in df.columns for c in categorical_candidates)\n",
    "has_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b03c1ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected OHE data or no categorical columns. Skipping ColumnTransformer.\n"
     ]
    }
   ],
   "source": [
    "# If data appears to be OHE already (no categorical columns), we'll skip ColumnTransformer.\n",
    "use_preprocessor = has_categorical\n",
    "\n",
    "if use_preprocessor:\n",
    "    numeric_features = [\"Duration_minutes\", \"stops_num\", \"Dep_Hour\", \"Arrival_Hour\", \"Journey_Day\", \"Journey_Month\"]\n",
    "    cat_features = [c for c in categorical_candidates if c in df.columns]\n",
    "    print(\"Using ColumnTransformer with numeric:\", numeric_features, \"and categorical:\", cat_features)\n",
    "\n",
    "    numeric_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, cat_features)\n",
    "    ], remainder=\"drop\")\n",
    "    # Feature names after transform will be built where needed.\n",
    "else:\n",
    "    print(\"Detected OHE data or no categorical columns. Skipping ColumnTransformer.\")\n",
    "    preprocessor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "817b8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepare X and Y\n",
    "X_all = df.drop(columns=[\"Price\", \"Price_capped\", \"log_Price\"], errors=\"ignore\")\n",
    "y_all = df[\"log_Price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "33c1ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7477, 26) Test shape: (3205, 26)\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3e02cf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to run: ['LinearRegression', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Models to train ----------\n",
    "models = {\n",
    "    \"LinearRegression\": {\"type\": \"sklearn\", \"est\": LinearRegression()},\n",
    "    \"RandomForest\": {\"type\": \"sklearn\", \"est\": RandomForestRegressor(n_estimators=150, max_depth=12, random_state=RANDOM_STATE, n_jobs=-1)},\n",
    "    \"GradientBoosting\": {\"type\": \"sklearn\", \"est\": GradientBoostingRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=RANDOM_STATE)}\n",
    "}\n",
    "if xgb_available:\n",
    "    models[\"XGBoost\"] = {\"type\": \"booster\", \"est\": XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, objective=\"reg:squarederror\", random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)}\n",
    "if lgb_available:\n",
    "    models[\"LightGBM\"] = {\"type\": \"booster\", \"est\": LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, n_jobs=-1)}\n",
    "if cb_available:\n",
    "    models[\"CatBoost\"] = {\"type\": \"booster\", \"est\": CatBoostRegressor(iterations=200, learning_rate=0.05, depth=6, random_state=RANDOM_STATE, verbose=False)}\n",
    "\n",
    "print(\"Models to run:\", list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "014e6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LinearRegression (type=sklearn) ...\n",
      "LinearRegression done — RMSE_price: 2625.62, MAE_price: 1826.49, time: 0.0s\n",
      "\n",
      "Training RandomForest (type=sklearn) ...\n",
      "RandomForest done — RMSE_price: 1723.00, MAE_price: 1170.91, time: 0.3s\n",
      "\n",
      "Training GradientBoosting (type=sklearn) ...\n",
      "GradientBoosting done — RMSE_price: 1707.48, MAE_price: 1216.72, time: 1.0s\n",
      "\n",
      "Training XGBoost (type=booster) ...\n",
      "XGBoost: fitted using method -> fallback_no_early_stopping\n",
      "XGBoost done — RMSE_price: 1689.42, MAE_price: 1191.51, time: 0.3s\n",
      "\n",
      "Training LightGBM (type=booster) ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 343\n",
      "[LightGBM] [Info] Number of data points in the train set: 5981, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 8.991873\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's l2: 0.0327995\n",
      "LightGBM: fitted using method -> lightgbm.callback.early_stopping\n",
      "LightGBM done — RMSE_price: 1740.74, MAE_price: 1234.31, time: 0.4s\n",
      "\n",
      "Training CatBoost (type=booster) ...\n",
      "CatBoost: fitted using method -> fit(early_stopping_rounds)\n",
      "CatBoost done — RMSE_price: 1876.62, MAE_price: 1350.06, time: 0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------- TRAIN & EVAL ----------\n",
    "trained_pipelines = {}\n",
    "results = []\n",
    "\n",
    "for name, info in models.items():\n",
    "    model = info[\"est\"]\n",
    "    mtype = info[\"type\"]\n",
    "    print(f\"\\nTraining {name} (type={mtype}) ...\")\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if mtype == \"sklearn\":\n",
    "            # For sklearn models fit on full outer training set\n",
    "            if preprocessor is not None:\n",
    "                pipe = Pipeline([(\"preprocessor\", preprocessor), (\"reg\", model)])\n",
    "                pipe.fit(X_train, y_train)\n",
    "                preds_log = pipe.predict(X_test)\n",
    "                final_pipeline = pipe  # ready to predict on raw rows\n",
    "            else:\n",
    "                # If no preprocessor, wrap scaler+lr only for LR\n",
    "                if name == \"LinearRegression\":\n",
    "                    lr_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", model)])\n",
    "                    lr_pipe.fit(X_train, y_train)\n",
    "                    preds_log = lr_pipe.predict(X_test)\n",
    "                    final_pipeline = lr_pipe\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    preds_log = model.predict(X_test)\n",
    "                    final_pipeline = Pipeline([(\"reg\", model)])\n",
    "\n",
    "        else:\n",
    "            # boosters: inner split for early stopping\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=RANDOM_STATE)\n",
    "            if preprocessor is not None:\n",
    "                # fit preprocessor on X_tr then transform\n",
    "                preprocessor.fit(X_tr)\n",
    "                X_tr_t = preprocessor.transform(X_tr)\n",
    "                X_val_t = preprocessor.transform(X_val)\n",
    "                X_test_t = preprocessor.transform(X_test)\n",
    "                # robust fit with fallback\n",
    "                fitted_booster, method = fit_booster_with_fallback(name, model, X_tr_t, y_tr, X_val_t, y_val)\n",
    "                print(f\"{name}: fitted using method -> {method}\")\n",
    "                preds_log = fitted_booster.predict(X_test_t)\n",
    "                final_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"reg\", fitted_booster)])\n",
    "            else:\n",
    "                X_tr_arr = X_tr.values if hasattr(X_tr, \"values\") else X_tr\n",
    "                X_val_arr = X_val.values if hasattr(X_val, \"values\") else X_val\n",
    "                X_test_arr = X_test.values if hasattr(X_test, \"values\") else X_test\n",
    "                fitted_booster, method = fit_booster_with_fallback(name, model, X_tr_arr, y_tr, X_val_arr, y_val)\n",
    "                print(f\"{name}: fitted using method -> {method}\")\n",
    "                preds_log = fitted_booster.predict(X_test_arr)\n",
    "                final_pipeline = Pipeline([(\"reg\", fitted_booster)])\n",
    "\n",
    "        # Evaluate\n",
    "        metrics = evaluate_predictions_log(y_test, preds_log)\n",
    "        metrics.update({\"Model\": name, \"time_sec\": time.time() - t0})\n",
    "        results.append(metrics)\n",
    "        trained_pipelines[name] = final_pipeline\n",
    "        print(f\"{name} done — RMSE_price: {metrics['rmse_price']:.2f}, MAE_price: {metrics['mae_price']:.2f}, time: {metrics['time_sec']:.1f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        results.append({\"Model\": name, \"rmse_log\": None, \"mae_log\": None, \"r2_log\": None, \"rmse_price\": None, \"mae_price\": None, \"r2_price\": str(e), \"time_sec\": None})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "922776fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results).sort_values(\"rmse_price\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7df14f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= MODEL PERFORMANCE COMPARISON =========\n",
      "              Model   rmse_price    mae_price  r2_price  time_sec\n",
      "0           XGBoost  1689.419908  1191.505379  0.844077  0.258721\n",
      "1  GradientBoosting  1707.481507  1216.724772  0.840726  0.955862\n",
      "2      RandomForest  1722.998261  1170.914918  0.837818  0.302119\n",
      "3          LightGBM  1740.739690  1234.314434  0.834460  0.441296\n",
      "4          CatBoost  1876.616660  1350.058798  0.807609  0.153472\n",
      "5  LinearRegression  2625.618777  1826.491241  0.623385  0.010539\n",
      "\n",
      "========= BEST MODEL SELECTED =========\n",
      "Best Model: XGBoost\n",
      "RMSE (Price): 1689.4199\n",
      "MAE  (Price): 1191.5054\n",
      "R²   (Price): 0.8441\n",
      "Training Time: 0.26 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n========= MODEL PERFORMANCE COMPARISON =========\")\n",
    "print(res_df[[\"Model\", \"rmse_price\", \"mae_price\", \"r2_price\", \"time_sec\"]])\n",
    "\n",
    "# Pick best model based on RMSE on original price scale\n",
    "best_idx = res_df[\"rmse_price\"].idxmin()\n",
    "best_model_name = res_df.loc[best_idx, \"Model\"]\n",
    "best_model = trained_pipelines[best_model_name]\n",
    "\n",
    "print(\"\\n========= BEST MODEL SELECTED =========\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"RMSE (Price): {res_df.loc[best_idx, 'rmse_price']:.4f}\")\n",
    "print(f\"MAE  (Price): {res_df.loc[best_idx, 'mae_price']:.4f}\")\n",
    "print(f\"R²   (Price): {res_df.loc[best_idx, 'r2_price']:.4f}\")\n",
    "print(f\"Training Time: {res_df.loc[best_idx, 'time_sec']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c014caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Duration_minutes',\n",
       " 'stops_num',\n",
       " 'Dep_Hour',\n",
       " 'Arrival_Hour',\n",
       " 'Journey_Day',\n",
       " 'Journey_Month',\n",
       " 'Airline_Air India',\n",
       " 'Airline_GoAir',\n",
       " 'Airline_IndiGo',\n",
       " 'Airline_Jet Airways',\n",
       " 'Airline_Jet Airways Business',\n",
       " 'Airline_Multiple carriers',\n",
       " 'Airline_Multiple carriers Premium economy',\n",
       " 'Airline_SpiceJet',\n",
       " 'Airline_Trujet',\n",
       " 'Airline_Vistara',\n",
       " 'Airline_Vistara Premium economy',\n",
       " 'Source_Chennai',\n",
       " 'Source_Delhi',\n",
       " 'Source_Kolkata',\n",
       " 'Source_Mumbai',\n",
       " 'Destination_Cochin',\n",
       " 'Destination_Delhi',\n",
       " 'Destination_Hyderabad',\n",
       " 'Destination_Kolkata',\n",
       " 'Destination_New Delhi']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_all.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8ce9f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"Duration_minutes\",\n",
    "    \"stops_num\",\n",
    "    \"Dep_Hour\",\n",
    "    \"Arrival_Hour\",\n",
    "    \"Journey_Day\",\n",
    "    \"Journey_Month\",\n",
    "]\n",
    "categorical_features = [\"Airline\", \"Source\", \"Destination\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ffcc4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d0a5cc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Airline'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/utils/_indexing.py:443\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     col_idx = all_columns.get_loc(col)\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers.Integral):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'Airline'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m final_pipeline = Pipeline(\n\u001b[32m      3\u001b[39m     steps=[\n\u001b[32m      4\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m\"\u001b[39m, final_preprocessor),\n\u001b[32m      5\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mreg\u001b[39m\u001b[33m\"\u001b[39m, models[best_model_name][\u001b[33m\"\u001b[39m\u001b[33mest\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      6\u001b[39m     ]\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Fit on FULL data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m final_pipeline.fit(X_all, y_all)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Sanity check (MUST NOT be None)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL PIPELINE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, final_pipeline)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28mself\u001b[39m._fit(X, y, routed_params, raw_params=params)\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = fit_transform_one_cached(\n\u001b[32m    590\u001b[39m     cloned_transformer,\n\u001b[32m    591\u001b[39m     X,\n\u001b[32m    592\u001b[39m     y,\n\u001b[32m    593\u001b[39m     weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    594\u001b[39m     message_clsname=\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    595\u001b[39m     message=\u001b[38;5;28mself\u001b[39m._log_message(step_idx),\n\u001b[32m    596\u001b[39m     params=step_params,\n\u001b[32m    597\u001b[39m )\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = transformer.fit_transform(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m, {}))\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:988\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    986\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_column_callables(X)\n\u001b[32m    989\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:541\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    539\u001b[39m         columns = columns(X)\n\u001b[32m    540\u001b[39m     all_columns.append(columns)\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     transformer_to_input_indices[name] = _get_column_indices(X, columns)\n\u001b[32m    543\u001b[39m \u001b[38;5;28mself\u001b[39m._columns = all_columns\n\u001b[32m    544\u001b[39m \u001b[38;5;28mself\u001b[39m._transformer_to_input_indices = transformer_to_input_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/utils/_indexing.py:451\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    448\u001b[39m         column_indices.append(col_idx)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mA given column is not a column of the dataframe\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[31mValueError\u001b[39m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "# Rebuild FINAL pipeline explicitly\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", final_preprocessor),\n",
    "        (\"reg\", models[best_model_name][\"est\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on FULL data\n",
    "final_pipeline.fit(X_all, y_all)\n",
    "\n",
    "# Sanity check (MUST NOT be None)\n",
    "print(\"FINAL PIPELINE:\\n\", final_pipeline)\n",
    "\n",
    "# Save\n",
    "os.makedirs(\"backend/model\", exist_ok=True)\n",
    "joblib.dump(final_pipeline, \"backend/model/flight_model.pkl\")\n",
    "\n",
    "print(\"✅ Saved correct pipeline with real preprocessor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d81988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rs in random_states:\n",
    "#     for ts in test_sizes:\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             X_all, y_all, test_size=ts, random_state=rs\n",
    "#         )\n",
    "\n",
    "#         for model_name, model in models.items():\n",
    "\n",
    "#             t0 = time.time()\n",
    "#             try:\n",
    "#                 model.fit(X_train, y_train)\n",
    "#                 preds = model.predict(X_test)\n",
    "#             except Exception as e:\n",
    "#                 preds = np.full_like(y_test, y_test.mean())  # fallback dummy prediction\n",
    "\n",
    "#             elapsed = time.time() - t0\n",
    "\n",
    "#             metrics = {\n",
    "#                 \"random_state\": rs,\n",
    "#                 \"test_size\": ts,\n",
    "#                 \"model\": model_name,\n",
    "#                 \"rmse\": root_mean_squared_error(y_test, preds),\n",
    "#                 \"mae\": mean_absolute_error(y_test, preds),\n",
    "#                 \"r2\": r2_score(y_test, preds),\n",
    "#                 \"train_time\": elapsed\n",
    "#             }\n",
    "\n",
    "#             results.append(metrics)\n",
    "\n",
    "# df_results = pd.DataFrame(results)\n",
    "# df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.to_csv(\"dashboard_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_idx = df_results[\"rmse\"].idxmin()\n",
    "# best_row = df_results.loc[best_idx]\n",
    "# best_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best performance at random_state={best_row.random_state}, \"\n",
    "#       f\"test_size={best_row.test_size}, model={best_row.model}, \"\n",
    "#       f\"RMSE={best_row.rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.to_json(\"dashboard_data.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import joblib\n",
    "\n",
    "# os.makedirs(\"backend/model\", exist_ok=True)\n",
    "\n",
    "# joblib.dump(best_model, \"backend/model/flight_model.pkl\")\n",
    "\n",
    "# print(f\"Saved best model: {best_model_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33436764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = joblib.load(\"backend/model/flight_model.pkl\")\n",
    "\n",
    "# sample_row = X_all.iloc[[0]]  # one row DataFrame\n",
    "# pred_log = loaded_model.predict(sample_row)[0]\n",
    "# pred_price = np.expm1(pred_log)\n",
    "\n",
    "# print(\"Prediction test (₹):\", int(pred_price))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f246ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL PIPELINE:\n",
      " Pipeline(steps=[('preprocessor', None),\n",
      "                ('reg',\n",
      "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "                              colsample_bylevel=None, colsample_bynode=None,\n",
      "                              colsample_bytree=0.8, device=None,\n",
      "                              early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              feature_types=None, feature_weights=None,\n",
      "                              gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None, learning_rate=0.05,\n",
      "                              max_bin=None, max_cat_threshold=None,\n",
      "                              max_cat_to_onehot=None, max_delta_step=None,\n",
      "                              max_depth=6, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=200, n_jobs=-1,\n",
      "                              num_parallel_tree=None, ...))])\n",
      "✅ Saved full pipeline with preprocessing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Ensure output directory\n",
    "os.makedirs(\"backend/model\", exist_ok=True)\n",
    "\n",
    "# Explicitly rebuild FINAL pipeline with preprocessor + best estimator\n",
    "best_estimator = models[best_model_name][\"est\"]\n",
    "\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),   # 🔴 THIS WAS MISSING\n",
    "        (\"reg\", best_estimator)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Refit on FULL DATA (important for production)\n",
    "final_pipeline.fit(X_all, y_all)\n",
    "\n",
    "# Sanity check (MUST show preprocessor)\n",
    "print(\"FINAL PIPELINE:\\n\", final_pipeline)\n",
    "\n",
    "# Save ONLY this pipeline\n",
    "joblib.dump(final_pipeline, \"backend/model/flight_model.pkl\")\n",
    "\n",
    "print(\"✅ Saved full pipeline with preprocessing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af212671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Pipeline(steps=[('reg',\n",
      "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "                              colsample_bylevel=None, colsample_bynode=None,\n",
      "                              colsample_bytree=0.8, device=None,\n",
      "                              early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              feature_types=None, feature_weights=None,\n",
      "                              gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None, learning_rate=0.05,\n",
      "                              max_bin=None, max_cat_threshold=None,\n",
      "                              max_cat_to_onehot=None, max_delta_step=None,\n",
      "                              max_depth=6, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=200, n_jobs=-1,\n",
      "                              num_parallel_tree=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "print(type(best_model))\n",
    "print(best_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "campusx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

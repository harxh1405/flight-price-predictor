{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "68c2b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (3.1.2)\n",
      "Requirement already satisfied: lightgbm in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: catboost in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (1.2.8)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from xgboost) (2.3.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from xgboost) (1.16.3)\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (3.10.7)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (2.3.3)\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (6.5.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/envs/campusx/lib/python3.12/site-packages (from plotly->catboost) (2.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "21a304a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "6a71f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_available = True\n",
    "except Exception:\n",
    "    xgb_available = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lgb_available = True\n",
    "except Exception:\n",
    "    lgb_available = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    cb_available = True\n",
    "except Exception:\n",
    "    cb_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "be2fc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ae4adfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##UTILITIES\n",
    "# def rmse(y_true, y_pred):\n",
    "#     return root_mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def evaluate_predictions_log(y_true_log, y_pred_log):\n",
    "    \"\"\"Return metrics on log scale and on original price scale (expm1).\"\"\"\n",
    "    # Log-target metrics\n",
    "    rmse_log = root_mean_squared_error(y_true_log, y_pred_log)\n",
    "    mae_log = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2_log = r2_score(y_true_log, y_pred_log)\n",
    "\n",
    "    # Convert back to price scale\n",
    "    y_true_price = np.expm1(y_true_log)\n",
    "    y_pred_price = np.expm1(y_pred_log)\n",
    "\n",
    "    rmse_price = root_mean_squared_error(y_true_price, y_pred_price)\n",
    "    mae_price = mean_absolute_error(y_true_price, y_pred_price)\n",
    "    r2_price = r2_score(y_true_price, y_pred_price)\n",
    "\n",
    "    return {\n",
    "        \"rmse_log\": rmse_log,\n",
    "        \"mae_log\": mae_log,\n",
    "        \"r2_log\": r2_log,\n",
    "        \"rmse_price\": rmse_price,\n",
    "        \"mae_price\": mae_price,\n",
    "        \"r2_price\": r2_price\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "57634431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_booster_with_fallback(name, model, X_tr_t, y_tr, X_val_t, y_val):\n",
    "    \"\"\"\n",
    "    Try multiple ways to fit booster with early stopping across library versions:\n",
    "      1) model.fit(..., eval_set=[...], early_stopping_rounds=...)\n",
    "      2) model.fit(..., eval_set=[...], callbacks=[...]) (XGBoost / LightGBM callback)\n",
    "      3) CatBoost: use_best_model=True\n",
    "      4) fallback: model.fit(...) without early stopping\n",
    "    Returns: (fitted_model, method_string)\n",
    "    \"\"\"\n",
    "    default_es = 30\n",
    "    # 1) try classic API\n",
    "    try:\n",
    "        model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], early_stopping_rounds=default_es, verbose=False)\n",
    "        return model, \"fit(early_stopping_rounds)\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) try callbacks for XGBoost/LightGBM\n",
    "    try:\n",
    "        if \"XGB\" in name and xgb_available:\n",
    "            try:\n",
    "                # attempt xgboost callback import\n",
    "                from xgboost.callback import EarlyStopping\n",
    "                model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[EarlyStopping(rounds=default_es)])\n",
    "                return model, \"xgb.callback.EarlyStopping\"\n",
    "            except Exception:\n",
    "                # alternate import path\n",
    "                import xgboost as xgbpkg\n",
    "                try:\n",
    "                    model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[xgbpkg.callback.EarlyStopping(rounds=default_es)])\n",
    "                    return model, \"xgbpkg.callback.EarlyStopping\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if \"LightGBM\" in name and lgb_available:\n",
    "            try:\n",
    "                from lightgbm import early_stopping\n",
    "                model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[early_stopping(default_es)])\n",
    "                return model, \"lightgbm.callback.early_stopping\"\n",
    "            except Exception:\n",
    "                # try package callback\n",
    "                try:\n",
    "                    import lightgbm as lgbpkg\n",
    "                    model.fit(X_tr_t, y_tr, eval_set=[(X_val_t, y_val)], callbacks=[lgbpkg.callback.early_stopping(default_es)])\n",
    "                    return model, \"lgbpkg.callback.early_stopping\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if \"CatBoost\" in name and cb_available:\n",
    "            try:\n",
    "                model.fit(X_tr_t, y_tr, eval_set=(X_val_t, y_val), use_best_model=True, verbose=False)\n",
    "                return model, \"catboost.use_best_model\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) fallback: fit without early stopping\n",
    "    try:\n",
    "        model.fit(X_tr_t, y_tr)\n",
    "        return model, \"fallback_no_early_stopping\"\n",
    "    except Exception as e:\n",
    "        # If it fails to fit at all, raise\n",
    "        raise RuntimeError(f\"Booster {name} failed to fit in all attempts: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "bb35e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10682, 29)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('processed_flight_data.csv')\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d8b00d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Detect whether data is OHE or needs preprocessing ----------\n",
    "# If categorical columns exist as names, we will build a ColumnTransformer.\n",
    "categorical_candidates = [\"Airline\", \"Source\", \"Destination\"]\n",
    "has_categorical = any(c in df.columns for c in categorical_candidates)\n",
    "has_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b03c1ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected OHE data or no categorical columns. Skipping ColumnTransformer.\n"
     ]
    }
   ],
   "source": [
    "# If data appears to be OHE already (no categorical columns), we'll skip ColumnTransformer.\n",
    "use_preprocessor = has_categorical\n",
    "\n",
    "if use_preprocessor:\n",
    "    numeric_features = [\"Duration_minutes\", \"stops_num\", \"Dep_Hour\", \"Arrival_Hour\", \"Journey_Day\", \"Journey_Month\"]\n",
    "    cat_features = [c for c in categorical_candidates if c in df.columns]\n",
    "    print(\"Using ColumnTransformer with numeric:\", numeric_features, \"and categorical:\", cat_features)\n",
    "\n",
    "    numeric_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, cat_features)\n",
    "    ], remainder=\"drop\")\n",
    "    # Feature names after transform will be built where needed.\n",
    "else:\n",
    "    print(\"Detected OHE data or no categorical columns. Skipping ColumnTransformer.\")\n",
    "    preprocessor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "817b8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepare X and Y\n",
    "X_all = df.drop(columns=[\"Price\", \"Price_capped\", \"log_Price\"], errors=\"ignore\")\n",
    "y_all = df[\"log_Price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "33c1ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7477, 26) Test shape: (3205, 26)\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3e02cf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to run: ['LinearRegression', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Models to train ----------\n",
    "models = {\n",
    "    \"LinearRegression\": {\"type\": \"sklearn\", \"est\": LinearRegression()},\n",
    "    \"RandomForest\": {\"type\": \"sklearn\", \"est\": RandomForestRegressor(n_estimators=150, max_depth=12, random_state=RANDOM_STATE, n_jobs=-1)},\n",
    "    \"GradientBoosting\": {\"type\": \"sklearn\", \"est\": GradientBoostingRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=RANDOM_STATE)}\n",
    "}\n",
    "if xgb_available:\n",
    "    models[\"XGBoost\"] = {\"type\": \"booster\", \"est\": XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, objective=\"reg:squarederror\", random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)}\n",
    "if lgb_available:\n",
    "    models[\"LightGBM\"] = {\"type\": \"booster\", \"est\": LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, n_jobs=-1)}\n",
    "if cb_available:\n",
    "    models[\"CatBoost\"] = {\"type\": \"booster\", \"est\": CatBoostRegressor(iterations=200, learning_rate=0.05, depth=6, random_state=RANDOM_STATE, verbose=False)}\n",
    "\n",
    "print(\"Models to run:\", list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "014e6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LinearRegression (type=sklearn) ...\n",
      "LinearRegression done — RMSE_price: 2625.62, MAE_price: 1826.49, time: 0.0s\n",
      "\n",
      "Training RandomForest (type=sklearn) ...\n",
      "RandomForest done — RMSE_price: 1723.00, MAE_price: 1170.91, time: 0.3s\n",
      "\n",
      "Training GradientBoosting (type=sklearn) ...\n",
      "GradientBoosting done — RMSE_price: 1707.48, MAE_price: 1216.72, time: 1.0s\n",
      "\n",
      "Training XGBoost (type=booster) ...\n",
      "XGBoost: fitted using method -> fallback_no_early_stopping\n",
      "XGBoost done — RMSE_price: 1689.42, MAE_price: 1191.51, time: 0.2s\n",
      "\n",
      "Training LightGBM (type=booster) ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 343\n",
      "[LightGBM] [Info] Number of data points in the train set: 5981, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 8.991873\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's l2: 0.0327995\n",
      "LightGBM: fitted using method -> lightgbm.callback.early_stopping\n",
      "LightGBM done — RMSE_price: 1740.74, MAE_price: 1234.31, time: 0.4s\n",
      "\n",
      "Training CatBoost (type=booster) ...\n",
      "CatBoost: fitted using method -> fit(early_stopping_rounds)\n",
      "CatBoost done — RMSE_price: 1876.62, MAE_price: 1350.06, time: 0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/campusx/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------- TRAIN & EVAL ----------\n",
    "trained_pipelines = {}\n",
    "results = []\n",
    "\n",
    "for name, info in models.items():\n",
    "    model = info[\"est\"]\n",
    "    mtype = info[\"type\"]\n",
    "    print(f\"\\nTraining {name} (type={mtype}) ...\")\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if mtype == \"sklearn\":\n",
    "            # For sklearn models fit on full outer training set\n",
    "            if preprocessor is not None:\n",
    "                pipe = Pipeline([(\"preprocessor\", preprocessor), (\"reg\", model)])\n",
    "                pipe.fit(X_train, y_train)\n",
    "                preds_log = pipe.predict(X_test)\n",
    "                final_pipeline = pipe  # ready to predict on raw rows\n",
    "            else:\n",
    "                # If no preprocessor, wrap scaler+lr only for LR\n",
    "                if name == \"LinearRegression\":\n",
    "                    lr_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", model)])\n",
    "                    lr_pipe.fit(X_train, y_train)\n",
    "                    preds_log = lr_pipe.predict(X_test)\n",
    "                    final_pipeline = lr_pipe\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    preds_log = model.predict(X_test)\n",
    "                    final_pipeline = Pipeline([(\"reg\", model)])\n",
    "\n",
    "        else:\n",
    "            # boosters: inner split for early stopping\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=RANDOM_STATE)\n",
    "            if preprocessor is not None:\n",
    "                # fit preprocessor on X_tr then transform\n",
    "                preprocessor.fit(X_tr)\n",
    "                X_tr_t = preprocessor.transform(X_tr)\n",
    "                X_val_t = preprocessor.transform(X_val)\n",
    "                X_test_t = preprocessor.transform(X_test)\n",
    "                # robust fit with fallback\n",
    "                fitted_booster, method = fit_booster_with_fallback(name, model, X_tr_t, y_tr, X_val_t, y_val)\n",
    "                print(f\"{name}: fitted using method -> {method}\")\n",
    "                preds_log = fitted_booster.predict(X_test_t)\n",
    "                final_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"reg\", fitted_booster)])\n",
    "            else:\n",
    "                X_tr_arr = X_tr.values if hasattr(X_tr, \"values\") else X_tr\n",
    "                X_val_arr = X_val.values if hasattr(X_val, \"values\") else X_val\n",
    "                X_test_arr = X_test.values if hasattr(X_test, \"values\") else X_test\n",
    "                fitted_booster, method = fit_booster_with_fallback(name, model, X_tr_arr, y_tr, X_val_arr, y_val)\n",
    "                print(f\"{name}: fitted using method -> {method}\")\n",
    "                preds_log = fitted_booster.predict(X_test_arr)\n",
    "                final_pipeline = Pipeline([(\"reg\", fitted_booster)])\n",
    "\n",
    "        # Evaluate\n",
    "        metrics = evaluate_predictions_log(y_test, preds_log)\n",
    "        metrics.update({\"Model\": name, \"time_sec\": time.time() - t0})\n",
    "        results.append(metrics)\n",
    "        trained_pipelines[name] = final_pipeline\n",
    "        print(f\"{name} done — RMSE_price: {metrics['rmse_price']:.2f}, MAE_price: {metrics['mae_price']:.2f}, time: {metrics['time_sec']:.1f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        results.append({\"Model\": name, \"rmse_log\": None, \"mae_log\": None, \"r2_log\": None, \"rmse_price\": None, \"mae_price\": None, \"r2_price\": str(e), \"time_sec\": None})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "922776fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results).sort_values(\"rmse_price\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7df14f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= MODEL PERFORMANCE COMPARISON =========\n",
      "              Model   rmse_price    mae_price  r2_price  time_sec\n",
      "0           XGBoost  1689.419908  1191.505379  0.844077  0.191951\n",
      "1  GradientBoosting  1707.481507  1216.724772  0.840726  0.953026\n",
      "2      RandomForest  1722.998261  1170.914918  0.837818  0.257384\n",
      "3          LightGBM  1740.739690  1234.314434  0.834460  0.401225\n",
      "4          CatBoost  1876.616660  1350.058798  0.807609  0.149789\n",
      "5  LinearRegression  2625.618777  1826.491241  0.623385  0.007508\n",
      "\n",
      "========= BEST MODEL SELECTED =========\n",
      "Best Model: XGBoost\n",
      "RMSE (Price): 1689.4199\n",
      "MAE  (Price): 1191.5054\n",
      "R²   (Price): 0.8441\n",
      "Training Time: 0.19 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n========= MODEL PERFORMANCE COMPARISON =========\")\n",
    "print(res_df[[\"Model\", \"rmse_price\", \"mae_price\", \"r2_price\", \"time_sec\"]])\n",
    "\n",
    "# Pick best model based on RMSE on original price scale\n",
    "best_idx = res_df[\"rmse_price\"].idxmin()\n",
    "best_model_name = res_df.loc[best_idx, \"Model\"]\n",
    "best_model = trained_pipelines[best_model_name]\n",
    "\n",
    "print(\"\\n========= BEST MODEL SELECTED =========\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"RMSE (Price): {res_df.loc[best_idx, 'rmse_price']:.4f}\")\n",
    "print(f\"MAE  (Price): {res_df.loc[best_idx, 'mae_price']:.4f}\")\n",
    "print(f\"R²   (Price): {res_df.loc[best_idx, 'r2_price']:.4f}\")\n",
    "print(f\"Training Time: {res_df.loc[best_idx, 'time_sec']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d81988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "campusx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
